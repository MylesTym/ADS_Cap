{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47e9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de1e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3d0af7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vectorizer = TfidfVectorizer(stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconsumer_complaint_narrative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2105\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2099\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2100\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2101\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2102\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2103\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2104\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2108\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:99\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     doc = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    101\u001b[39m     doc = analyzer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CFPB Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:232\u001b[39m, in \u001b[36m_VectorizerMixin.decode\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    229\u001b[39m     doc = doc.decode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[38;5;28mself\u001b[39m.decode_error)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np.nan:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[31mValueError\u001b[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Handle null values in text column\n",
    "df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].fillna('')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "x = vectorizer.fit_transform(df['consumer_complaint_narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd335dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 ## test diffrent variable amounts\n",
    "kmeans = KMeans(n_clusters=k, random_state=25)\n",
    "labels = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b14b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0 top terms:\n",
      "credit, report, information, xxxx, reporting, identity, accounts, theft, account, inquiries, \n",
      "Cluster 1 top terms:\n",
      "consumer, xxxx, information, 15, reporting, report, consent, code, agency, reports, \n",
      "Cluster 2 top terms:\n",
      "section, usc, 15, 1681, consumer, states, xxxx, reporting, agency, furnish, \n",
      "Cluster 3 top terms:\n",
      "xxxx, xxxxxxxx, account, credit, report, information, reporting, accounts, balance, date, \n",
      "Cluster 4 top terms:\n",
      "debt, xxxx, collection, company, credit, validation, account, proof, alleged, report, \n",
      "Cluster 5 top terms:\n",
      "accounts, credit, report, inaccurate, duty, litigation, fraudulent, accordingly, deny, inaccuracies, \n",
      "Cluster 6 top terms:\n",
      "xxxx, xxxxxxxx, credit, report, items, information, consumer, balance, account, reporting, \n",
      "Cluster 7 top terms:\n",
      "xxxx, account, bank, card, payment, loan, xxxxxxxx, told, credit, called, \n",
      "Cluster 8 top terms:\n",
      "letters, xxxx, complaint, filing, party, involved, im, uploaded, falsely, misleading, \n",
      "Cluster 9 top terms:\n",
      "late, payment, payments, xxxx, error, credit, account, reporting, report, consumer, "
     ]
    }
   ],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"\\nCluster {i} top terms:\")\n",
    "    for j in order_centroids[i, :10]:  # Top 10 terms\n",
    "        print(terms[j], end=\", \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
